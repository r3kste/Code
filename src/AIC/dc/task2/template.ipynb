{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DC Task 2\n",
        "\n",
        "Welcome to your second task! This will be implementing logistic regression. Follow the code cells given below and fill the empty code cells with the appropriate code. We will walk you through every step of the way. Ensure that you understand every block of code that we've provided before filling the empty blocks.\n",
        "\n",
        "The first step is to import useful libraries. `numpy`, `pandas` and `matplotlib` are very useful in data manipulation as discussed."
      ],
      "metadata": {
        "id": "f-FbJc62ltmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qgfSdANlikq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load the training and test datasets using `pd.read_csv()`."
      ],
      "metadata": {
        "id": "SRECZ-Vib2-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/car_train.csv')\n",
        "test_data = pd.read_csv('/content/car_test.csv')"
      ],
      "metadata": {
        "id": "trJJw59BbemA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation\n",
        "\n",
        "The first step in any kind of Machine Learing model building is data visualisation. It is important to have a feel for the data before we train any kind of model on it. This will help us understand what features are important, if there are any outliers, and if we have to clean some values up before we feed it into a model.\n",
        "\n",
        "Let's get started on some basic commands that may help you do this. An example is given below.\n",
        "\n",
        "1. What command helps you to sample 3 random rows from the data?\n",
        "\n",
        "Answer: `DataFrame.sample(3)`"
      ],
      "metadata": {
        "id": "lqaK0513ccPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.sample(3)"
      ],
      "metadata": {
        "id": "u5iUeheMeP1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Commands Questions\n",
        "\n",
        "1. Write a command to look at the first seven rows of the data.\n",
        "2. Find a pandas command that tells you the mean, avg, count, max qualities of every numeric column.\n",
        "3. Write a command to find all the unique values in `Car_Acceptability` and the `Buying_Price` column.\n",
        "4. Write a command to count the number of rows in the dataset."
      ],
      "metadata": {
        "id": "AkedWCtiwknI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3xtWTJ-fW1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erEh-UA0xMl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGCFDIsAxMbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-Vf-q0dxMJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Data\n",
        "\n",
        "Now that we've gone over some useful basic pandas commands, let's dive a little deeper. We are very interested in understanding the correlation between different columns in this data. One example is given below for your benefit.\n",
        "\n",
        "1. How does the `Person_Capacity` depend on the `Buying_Price`?"
      ],
      "metadata": {
        "id": "qzdZr6Z7gLtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for price in train_data['Buying_Price'].unique():\n",
        "  person_cap = train_data.loc[train_data['Buying_Price'] == f'{price}'].Person_Capacity.value_counts()\n",
        "\n",
        "  sns.barplot(y = person_cap, x = person_cap.index)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hXp6-TKAi5II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the below questions with the help of `matplotlib` and use the plots to enhance your understanding of the data.\n",
        "\n",
        "1. How does the `Car_Acceptability` depend on the `Buying_Price`? Plot and explain your observations briefly.\n",
        "2. How does the `Car_Acceptability` depend on the `Safety`? That should give you an idea about the priority that users give to safety while buying a car.\n",
        "3. Show the number of cars in each category of acceptability in `Car_Acceptability` using a barplot. What consequences can this have on your model?\n",
        "4. How does the `Bluebook_score` depend on `Person_Capacity`?"
      ],
      "metadata": {
        "id": "Sqat0DiW3zc4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AIGZe_XyOZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPwWMlwHyOKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iD01UFJaj84Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMbX97QFQfiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do it Yourself!\n",
        "\n",
        "Explore the intricacies of the data yourself using the four questions above as a starting point. Think about what plots could help you extract the most meaningful information about the data and how this might help your overall prediction accuracy. Make sure to write your inferences (in comments) after each plot."
      ],
      "metadata": {
        "id": "3uq6dtJdkQlP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iC0KXXbl48K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8o2ny0Hl5lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3_JqlzNl5bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Now that we've understood all the gossip between each of the columns in the dataset, we need to process each feature to ensure that they are appropriate to feed into the model. Some important aspects of this are encoding categorical data, removing duplicate data and redundant features, normalization of values, adjusting class imbalances, etc.\n",
        "\n",
        "Python libraries like `sklearn` can be very helpful in assisting you. Please feel free to look up different ways of preprocessing that might apply here.\n",
        "\n",
        "\n",
        "Note: Do remember to preprocess your test data as well for inference!\n"
      ],
      "metadata": {
        "id": "K46OERIFmJ3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Which columns in the dataset have missing values?\n",
        "\n",
        "cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
        "print(cols_with_missing)\n",
        "\n",
        "# Which columns are categorical and need to be encoded?\n",
        "\n",
        "categorical_cols = [col for col in train_data.columns if train_data[col].dtype == \"object\"]\n",
        "print(categorical_cols)\n",
        "\n",
        "# Example for (ordinal) encoding categorical columns\n",
        "\n",
        "train_data['Buying_Price'].replace(['low','med','high', 'vhigh'], [1,2,3,4], inplace=True)\n",
        "\n",
        "# Do the same for the test data"
      ],
      "metadata": {
        "id": "Duq_Hqu9mJNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyJOiFezp35v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model\n",
        "\n",
        "Now's the time to finally build a logistic regression model for classification. You may use `scikit-learn` for a simplified implementation. Measure and print your accuracy once you inference your predictions. If you would like to not use `sklearn` then please feel free to delete the below code cell.\n",
        "\n",
        "Also, print the confusion matrix for all the classes and calculate the recall, precision and F1 score for each class."
      ],
      "metadata": {
        "id": "31HaNP9fru_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# create a LogisticRegression() object with appropriate parameters like regularization type"
      ],
      "metadata": {
        "id": "EEwKEME3tMbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FW1vDxfltNga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXnPmoEItM-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "That's it! Hope you've gotten a good idea of how data is visualized, processed and then trained upon by a model to make predictions. Feel free to reach out if you have any doubts because fundamentally any learning process requires interaction."
      ],
      "metadata": {
        "id": "vRXV0xFc0vxK"
      }
    }
  ]
}